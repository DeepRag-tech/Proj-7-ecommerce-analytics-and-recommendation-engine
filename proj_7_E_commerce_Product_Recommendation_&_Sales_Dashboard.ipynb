{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1: Business Understanding (The Foundation)**\n",
        "\n",
        "Before we download the data, let's formally state the first phase we have just completed. This ensures our technical work is always guided by a clear business purpose.\n",
        "\n",
        "*   **Objective:** To help an e-commerce business increase its profitability and customer retention.\n",
        "*   **Business Problem:** The marketing and sales teams currently lack the insights needed for data-driven decision-making. They don't have a clear view of top-selling products, high-value customers, or cross-selling opportunities.\n",
        "*   **Project Goal:** To build a centralized dashboard and recommendation system that provides actionable insights for sales analysis, customer segmentation, and product promotions. This will be our \"single source of truth.\"\n",
        "*   **Success Criteria:** The project is successful if we deliver a live dashboard, a working recommendation function, and a well-documented GitHub repository."
      ],
      "metadata": {
        "id": "UnNINuEjHeur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEITj3NmHbfa",
        "outputId": "b3405c70-8066-43b5-94ae-8400d242cb9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "WNnj5jO4HbaZ",
        "outputId": "928b0cd4-cd2c-44a8-cd85-66f13209817f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6f245d89-3783-44fc-9cac-b2be05ba716a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6f245d89-3783-44fc-9cac-b2be05ba716a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"\\nKaggle API configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGZZJ7wiHbSh",
        "outputId": "328fc8a1-6101-4bd4-c437-8832184acd1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kaggle API configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wsnl6vEPnEj",
        "outputId": "e8a58651-92bb-46d8-f89e-7d1171840742"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                        title                                                    size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------  -------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "mosapabdelghany/medical-insurance-cost-dataset             Medical Insurance Cost Dataset                          16425  2025-08-24 11:54:36.533000          15145        298  1.0              \n",
            "zadafiyabhrami/global-crocodile-species-dataset            Global Crocodile Species Dataset                        57473  2025-08-26 08:46:11.950000           9990        282  1.0              \n",
            "codebynadiia/gdp-per-country-20202025                      GDP per Country 2020‚Äì2025                                5677  2025-09-04 14:37:43.563000           8204        156  1.0              \n",
            "saadaliyaseen/analyzing-student-academic-trends            Analyzing Student Academic Trends                        2430  2025-09-10 15:19:31.970000           4469        110  1.0              \n",
            "yashdevladdha/uber-ride-analytics-dashboard                Uber Data Analytics Dashboard                        17324552  2025-08-08 11:13:42.920000          55352       1177  1.0              \n",
            "mjshubham21/movie-dataset-for-analytics-and-visualization  Movie Dataset for Analytics & Visualization          65136772  2025-09-08 12:13:22.823000           2114         29  1.0              \n",
            "adharshinikumar/screentime-vs-mentalwellness-survey-2025   Screen Time vs Mental Wellness Survey - 2025            10214  2025-09-14 15:49:10.683000           1844         29  1.0              \n",
            "nabihazahid/spotify-dataset-for-churn-analysis             Spotify Analysis Dataset 2025                           99163  2025-08-28 23:44:35.173000           3631         46  1.0              \n",
            "mosapabdelghany/adult-income-prediction-dataset            Adult Income Prediction Dataset                        460936  2025-08-28 11:56:47.150000           2096         24  1.0              \n",
            "vrajesh0sharma7/used-car-price-prediction                  Used Car Price Prediction                              382872  2025-09-19 12:16:19.927000           1651         25  0.9411765        \n",
            "wardabilal/salary-prediction-dataset                       Salary Prediction Dataset                               17048  2025-09-06 14:14:03.303000           2229         31  1.0              \n",
            "hubertsidorowicz/football-players-stats-2025-2026          Football Players Stats (2025-2026)                     724697  2025-09-22 12:26:30.963000           1095         26  1.0              \n",
            "msnbehdani/mock-dataset-of-second-hand-car-sales           Car Sales Dataset: Model, Features, and Pricing        501188  2025-08-20 17:47:58.207000           8448        114  1.0              \n",
            "pinuto/us-oil-and-gas-production-and-disposition-20152025  üìä US Oil & Gas Production & Disposition 2015‚Äì2025     7970935  2025-09-18 09:57:04.747000            919         23  1.0              \n",
            "nabeelqureshitiii/student-performance-dataset              Student Performance Dataset                           9364133  2025-08-27 11:28:12.570000           3495         59  1.0              \n",
            "vedikagupta0/imdb-top-950-movies-dataset-2025              üé¨ IMDb Top 950 Movies Dataset (2025)                   507681  2025-09-09 06:07:20                  1021         21  0.88235295       \n",
            "ayeshaimran123/academic-stress-level-maintenance-dataset   Academic Stress Level                                    2104  2025-09-12 07:46:59.777000           1248         33  1.0              \n",
            "zubairamuti/shopping-behaviours-dataset                    Shopping behaviours dataset                             72157  2025-08-29 14:56:21.637000           3939         61  1.0              \n",
            "ahmadrazakashif/spotify-popularity-songs                   Spotify_Popularity.Songs                                48213  2025-09-12 10:16:24.563000           1248         35  1.0              \n",
            "varishabatool/data-set                                     Medical_Insurance cost Dataset                          16425  2025-09-10 09:33:41.840000            482         22  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d olistbr/brazilian-ecommerce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bzBOMCeP41C",
        "outputId": "beca94d4-692f-42db-84d9-ad8b9d49a575"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading brazilian-ecommerce.zip to /content\n",
            "  0% 0.00/42.6M [00:00<?, ?B/s]\n",
            "100% 42.6M/42.6M [00:00<00:00, 474MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip brazilian-ecommerce.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-3956ZrQVn0",
        "outputId": "56d4205b-2606-4ee7-c8c2-47a7f262602d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  brazilian-ecommerce.zip\n",
            "  inflating: olist_customers_dataset.csv  \n",
            "  inflating: olist_geolocation_dataset.csv  \n",
            "  inflating: olist_order_items_dataset.csv  \n",
            "  inflating: olist_order_payments_dataset.csv  \n",
            "  inflating: olist_order_reviews_dataset.csv  \n",
            "  inflating: olist_orders_dataset.csv  \n",
            "  inflating: olist_products_dataset.csv  \n",
            "  inflating: olist_sellers_dataset.csv  \n",
            "  inflating: product_category_name_translation.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataset files have been downloaded and unzipped:\")\n",
        "!ls *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9ngKM_Qnx6",
        "outputId": "39116b6f-fd19-4f9b-d54f-66f76f8a9c85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset files have been downloaded and unzipped:\n",
            "olist_customers_dataset.csv\t  olist_orders_dataset.csv\n",
            "olist_geolocation_dataset.csv\t  olist_products_dataset.csv\n",
            "olist_order_items_dataset.csv\t  olist_sellers_dataset.csv\n",
            "olist_order_payments_dataset.csv  product_category_name_translation.csv\n",
            "olist_order_reviews_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Data Understanding**\n",
        "\n",
        "The goal of this phase is to become familiar with the data. We're not cleaning or changing anything yet; we're just exploring.\n",
        "\n",
        "Our investigation will focus on three questions:\n",
        "1.  What data is in each file? (Column names, data types)\n",
        "2.  Are there any obvious quality issues? (Missing values, incorrect formats)\n",
        "3.  How do the files relate to each other?"
      ],
      "metadata": {
        "id": "bB2aWlJYR2Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "xGvAk9BaSBu_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={}\n",
        "for file in os.listdir(\".\"):\n",
        "  if file.endswith(\".csv\"):\n",
        "    column_names=file.replace(\"olist_\", \"\").replace(\"_dataset\",\"\").replace(\".csv\",\"\")\n",
        "    data[column_names]=pd.read_csv(file)\n",
        "    print(f\"Loaded '{file}' as data['{column_names}']\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3qSY_pLR3Nh",
        "outputId": "626f10fe-c6fb-4928-e36d-34bf4b019ec6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 'olist_order_payments_dataset.csv' as data['order_payments']\n",
            "Loaded 'olist_sellers_dataset.csv' as data['sellers']\n",
            "Loaded 'product_category_name_translation.csv' as data['product_category_name_translation']\n",
            "Loaded 'olist_order_reviews_dataset.csv' as data['order_reviews']\n",
            "Loaded 'olist_customers_dataset.csv' as data['customers']\n",
            "Loaded 'olist_products_dataset.csv' as data['products']\n",
            "Loaded 'olist_geolocation_dataset.csv' as data['geolocation']\n",
            "Loaded 'olist_orders_dataset.csv' as data['orders']\n",
            "Loaded 'olist_order_items_dataset.csv' as data['order_items']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have customers information, orders information, products and order items information\n",
        "# 1. Customers: Information about the customer\n",
        "print(\"\\n--- 1. Customers ---\")\n",
        "print(data['customers'].info())\n",
        "print(\"\\nMissing values in customers:\")\n",
        "print(data['customers'].isnull().sum())\n",
        "print(\"\\nSample customer data:\")\n",
        "print(data['customers'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkG_mGlmRFYa",
        "outputId": "496b78c1-cfa8-4672-fa3b-6ef6a8aeaeaf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. Customers ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99441 entries, 0 to 99440\n",
            "Data columns (total 5 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   customer_id               99441 non-null  object\n",
            " 1   customer_unique_id        99441 non-null  object\n",
            " 2   customer_zip_code_prefix  99441 non-null  int64 \n",
            " 3   customer_city             99441 non-null  object\n",
            " 4   customer_state            99441 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 3.8+ MB\n",
            "None\n",
            "\n",
            "Missing values in customers:\n",
            "customer_id                 0\n",
            "customer_unique_id          0\n",
            "customer_zip_code_prefix    0\n",
            "customer_city               0\n",
            "customer_state              0\n",
            "dtype: int64\n",
            "\n",
            "Sample customer data:\n",
            "                        customer_id                customer_unique_id  \\\n",
            "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
            "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
            "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
            "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
            "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
            "\n",
            "   customer_zip_code_prefix          customer_city customer_state  \n",
            "0                     14409                 franca             SP  \n",
            "1                      9790  sao bernardo do campo             SP  \n",
            "2                      1151              sao paulo             SP  \n",
            "3                      8775        mogi das cruzes             SP  \n",
            "4                     13056               campinas             SP  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Orders: Connects customers to their purchases\n",
        "print(\"\\n\\n--- 2. Orders ---\")\n",
        "print(data['orders'].info())\n",
        "print(\"\\nMissing values in orders:\")\n",
        "print(data['orders'].isnull().sum())\n",
        "print(\"\\nSample order data:\")\n",
        "print(data['orders'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_MI0OJCUlTY",
        "outputId": "c2e847e5-d779-423d-c634-63af8fcb832b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- 2. Orders ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99441 entries, 0 to 99440\n",
            "Data columns (total 8 columns):\n",
            " #   Column                         Non-Null Count  Dtype \n",
            "---  ------                         --------------  ----- \n",
            " 0   order_id                       99441 non-null  object\n",
            " 1   customer_id                    99441 non-null  object\n",
            " 2   order_status                   99441 non-null  object\n",
            " 3   order_purchase_timestamp       99441 non-null  object\n",
            " 4   order_approved_at              99281 non-null  object\n",
            " 5   order_delivered_carrier_date   97658 non-null  object\n",
            " 6   order_delivered_customer_date  96476 non-null  object\n",
            " 7   order_estimated_delivery_date  99441 non-null  object\n",
            "dtypes: object(8)\n",
            "memory usage: 6.1+ MB\n",
            "None\n",
            "\n",
            "Missing values in orders:\n",
            "order_id                            0\n",
            "customer_id                         0\n",
            "order_status                        0\n",
            "order_purchase_timestamp            0\n",
            "order_approved_at                 160\n",
            "order_delivered_carrier_date     1783\n",
            "order_delivered_customer_date    2965\n",
            "order_estimated_delivery_date       0\n",
            "dtype: int64\n",
            "\n",
            "Sample order data:\n",
            "                           order_id                       customer_id  \\\n",
            "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
            "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
            "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
            "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
            "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
            "\n",
            "  order_status order_purchase_timestamp    order_approved_at  \\\n",
            "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
            "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
            "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
            "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
            "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
            "\n",
            "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
            "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
            "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
            "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
            "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
            "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
            "\n",
            "  order_estimated_delivery_date  \n",
            "0           2017-10-18 00:00:00  \n",
            "1           2018-08-13 00:00:00  \n",
            "2           2018-09-04 00:00:00  \n",
            "3           2017-12-15 00:00:00  \n",
            "4           2018-02-26 00:00:00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Order Items: Details of what was in each order (products, prices)\n",
        "print(\"\\n\\n--- 3. Order Items ---\")\n",
        "print(data['order_items'].info())\n",
        "print(\"\\nMissing values in order_items:\")\n",
        "print(data['order_items'].isnull().sum())\n",
        "print(\"\\nSample order items data:\")\n",
        "print(data['order_items'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rtf1x_7U4dx",
        "outputId": "a7588675-e7fc-4ab0-abc6-e01715692c17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- 3. Order Items ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 112650 entries, 0 to 112649\n",
            "Data columns (total 7 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   order_id             112650 non-null  object \n",
            " 1   order_item_id        112650 non-null  int64  \n",
            " 2   product_id           112650 non-null  object \n",
            " 3   seller_id            112650 non-null  object \n",
            " 4   shipping_limit_date  112650 non-null  object \n",
            " 5   price                112650 non-null  float64\n",
            " 6   freight_value        112650 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(4)\n",
            "memory usage: 6.0+ MB\n",
            "None\n",
            "\n",
            "Missing values in order_items:\n",
            "order_id               0\n",
            "order_item_id          0\n",
            "product_id             0\n",
            "seller_id              0\n",
            "shipping_limit_date    0\n",
            "price                  0\n",
            "freight_value          0\n",
            "dtype: int64\n",
            "\n",
            "Sample order items data:\n",
            "                           order_id  order_item_id  \\\n",
            "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
            "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
            "2  000229ec398224ef6ca0657da4fc703e              1   \n",
            "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
            "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
            "\n",
            "                         product_id                         seller_id  \\\n",
            "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
            "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
            "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
            "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
            "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
            "\n",
            "   shipping_limit_date   price  freight_value  \n",
            "0  2017-09-19 09:45:35   58.90          13.29  \n",
            "1  2017-05-03 11:05:13  239.90          19.93  \n",
            "2  2018-01-18 14:48:30  199.00          17.87  \n",
            "3  2018-08-15 10:10:18   12.99          12.79  \n",
            "4  2017-02-13 13:57:51  199.90          18.14  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Products: Details about the products themselves\n",
        "print(\"\\n\\n--- 4. Products ---\")\n",
        "print(data['products'].info())\n",
        "print(\"\\nMissing values in products:\")\n",
        "print(data['products'].isnull().sum())\n",
        "print(\"\\nSample products data:\")\n",
        "print(data['products'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjmVqzKvVSrK",
        "outputId": "2841cbba-2289-4783-9407-36e5a0b3fa09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- 4. Products ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32951 entries, 0 to 32950\n",
            "Data columns (total 9 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   product_id                  32951 non-null  object \n",
            " 1   product_category_name       32341 non-null  object \n",
            " 2   product_name_lenght         32341 non-null  float64\n",
            " 3   product_description_lenght  32341 non-null  float64\n",
            " 4   product_photos_qty          32341 non-null  float64\n",
            " 5   product_weight_g            32949 non-null  float64\n",
            " 6   product_length_cm           32949 non-null  float64\n",
            " 7   product_height_cm           32949 non-null  float64\n",
            " 8   product_width_cm            32949 non-null  float64\n",
            "dtypes: float64(7), object(2)\n",
            "memory usage: 2.3+ MB\n",
            "None\n",
            "\n",
            "Missing values in products:\n",
            "product_id                      0\n",
            "product_category_name         610\n",
            "product_name_lenght           610\n",
            "product_description_lenght    610\n",
            "product_photos_qty            610\n",
            "product_weight_g                2\n",
            "product_length_cm               2\n",
            "product_height_cm               2\n",
            "product_width_cm                2\n",
            "dtype: int64\n",
            "\n",
            "Sample products data:\n",
            "                         product_id  product_category_name  \\\n",
            "0  1e9e8ef04dbcff4541ed26657ea517e5             perfumaria   \n",
            "1  3aa071139cb16b67ca9e5dea641aaa2f                  artes   \n",
            "2  96bd76ec8810374ed1b65e291975717f          esporte_lazer   \n",
            "3  cef67bcfe19066a932b7673e239eb23d                  bebes   \n",
            "4  9dc1a7de274444849c219cff195d0b71  utilidades_domesticas   \n",
            "\n",
            "   product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
            "0                 40.0                       287.0                 1.0   \n",
            "1                 44.0                       276.0                 1.0   \n",
            "2                 46.0                       250.0                 1.0   \n",
            "3                 27.0                       261.0                 1.0   \n",
            "4                 37.0                       402.0                 4.0   \n",
            "\n",
            "   product_weight_g  product_length_cm  product_height_cm  product_width_cm  \n",
            "0             225.0               16.0               10.0              14.0  \n",
            "1            1000.0               30.0               18.0              20.0  \n",
            "2             154.0               18.0                9.0              15.0  \n",
            "3             371.0               26.0                4.0              26.0  \n",
            "4             625.0               20.0               17.0              13.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 3: Data Preparation**\n",
        "\n",
        "This is the Extract, Transform, Load (ETL) phase. I want to experience the practical steps followed in company so I am gonna save the data from its current messy state in CSV files into a clean, structured, and permanent home in a cloud database. A relational database like PostgreSQL is the ideal destination because it's built to handle the kind of multi-table relationships we see here. It has both free and paid. well, for practice purpose I will go with the free one."
      ],
      "metadata": {
        "id": "5dqlhAJGWTYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you see here, I have installed psycopg2-binary. Why I did that? Basically it connects python\n",
        "# to a PostgreSQL and it will allow me to run sql queries direclty from python. It means it allows\n",
        "# python to communicate with the database. You will find how it works in further steps.\n",
        "!pip install psycopg2-binary SQLAlchemy\n",
        "print(\"Installed required libraries\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMioYeMoWQ22",
        "outputId": "7c470e48-7dda-4bab-ddab-2952e9f870cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (4.15.0)\n",
            "Downloading psycopg2_binary-2.9.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/3.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n",
            "Installed required libraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The thing is that it is difficult to mention why I use all these libraries.\n",
        "# I will do one thing. I will put everything in the readme file on github okay. you can check there.\n",
        "# Don't blindly copy paste the code blocks. Just go through each block to understand what we did.\n",
        "from sqlalchemy import create_engine, inspect\n",
        "import glob\n",
        "from google.colab import userdata\n",
        "import time\n",
        "print(\"Modules imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM-td--v9FQJ",
        "outputId": "f6723559-848e-4864-c3bc-0ad9396bc040"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modules imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    db_connection_str_raw = userdata.get('DB_CONNECTION_STRING')\n",
        "    print(\"Successfully retrieved raw database connection string.\")\n",
        "\n",
        "    # --- DEBUGGING AND SANITIZING ---\n",
        "    # 1. Strip any leading/trailing whitespace\n",
        "    db_connection_str = db_connection_str_raw.strip()\n",
        "\n",
        "    # 2. Ensure it starts with the correct prefix\n",
        "    if db_connection_str.startswith(\"postgres://\"):\n",
        "        db_connection_str = db_connection_str.replace(\"postgres://\", \"postgresql://\", 1)\n",
        "\n",
        "    print(\"Sanitized connection string. Ready to create engine.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving secret. Please ensure you have set 'DB_CONNECTION_STRING' in Colab secrets. Error: {e}\")\n",
        "    raise\n",
        "print(\"Connected successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey63e4ia9Y26",
        "outputId": "ca0ed9ce-489f-42c6-8c83-b8874b668a30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully retrieved raw database connection string.\n",
            "Sanitized connection string. Ready to create engine.\n",
            "Connected successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    engine = create_engine(db_connection_str)\n",
        "    # Test the connection immediately\n",
        "    with engine.connect() as connection:\n",
        "        print(\"‚úÖ Database engine created and connection successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating engine or connecting to the database: {e}\")\n",
        "    print(\"\\n--- TROUBLESHOOTING ---\")\n",
        "    print(\"1. Double-check your DB_CONNECTION_STRING in Colab Secrets. Make sure there are no typos.\")\n",
        "    print(\"2. Verify that your Neon database is active and not suspended.\")\n",
        "    print(\"3. Check that your password does not contain special characters that need URL encoding (e.g., @, :, /). If it does, you may need to generate a new password in Neon.\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta8f9Ssx-bWJ",
        "outputId": "f31c03c7-d185-4628-e3bc-5cb6097f34a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Database engine created and connection successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_name_map = {\n",
        "    'olist_customers_dataset.csv': 'customers',\n",
        "    'olist_geolocation_dataset.csv': 'geolocation',\n",
        "    'olist_order_items_dataset.csv': 'order_items',\n",
        "    'olist_order_payments_dataset.csv': 'order_payments',\n",
        "    'olist_order_reviews_dataset.csv': 'order_reviews',\n",
        "    'olist_orders_dataset.csv': 'orders',\n",
        "    'olist_products_dataset.csv': 'products',\n",
        "    'olist_sellers_dataset.csv': 'sellers',\n",
        "    'product_category_name_translation.csv': 'product_category_translation'\n",
        "}\n",
        "\n",
        "print(\"Schema definition complete.\")\n",
        "\n",
        "# --- 3. EXECUTE ETL: Loop through files and load to database ---\n",
        "\n",
        "print(\"\\n--- Starting the main ETL Process ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Get a list of all CSV files in the current Colab directory\n",
        "csv_files = glob.glob('*.csv')\n",
        "print(csv_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTK_mmP8Ewc2",
        "outputId": "1ead86e7-4af2-4591-87d3-82cb7fc3b2e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema definition complete.\n",
            "\n",
            "--- Starting the main ETL Process ---\n",
            "['olist_order_payments_dataset.csv', 'olist_sellers_dataset.csv', 'product_category_name_translation.csv', 'olist_order_reviews_dataset.csv', 'olist_customers_dataset.csv', 'olist_products_dataset.csv', 'olist_geolocation_dataset.csv', 'olist_orders_dataset.csv', 'olist_order_items_dataset.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_path in csv_files:\n",
        "    file_name = os.path.basename(file_path) # e.g., 'olist_customers_dataset.csv'\n",
        "\n",
        "    # Process only the files we've defined in our schema map\n",
        "    if file_name in table_name_map:\n",
        "        table_name = table_name_map[file_name]\n",
        "        print(f\"\\nProcessing file: '{file_name}'...\")\n",
        "\n",
        "        # -----------------\n",
        "        # E - EXTRACT\n",
        "        # -----------------\n",
        "        # Read the raw data from the CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"   (1/3) Extracted {len(df)} rows.\")\n",
        "\n",
        "        # -----------------\n",
        "        # T - TRANSFORM\n",
        "        # -----------------\n",
        "        # The primary transformation is correcting the data types of date columns.\n",
        "        # This is a critical data cleaning step for this specific dataset.\n",
        "        for col in df.columns:\n",
        "            if '_date' in col or '_timestamp' in col:\n",
        "                # This line converts a text column to a proper datetime object.\n",
        "                # If a value cannot be converted, it becomes 'NaT' (Not a Time).\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        print(\"   (2/3) Transformed data types for date columns.\")\n",
        "\n",
        "\n",
        "        # -----------------\n",
        "        # L - LOAD\n",
        "        # -----------------\n",
        "        # This function writes the cleaned DataFrame to a table in your PostgreSQL database.\n",
        "        # `if_exists='replace'` will drop the table if it already exists and create a new one.\n",
        "        # `index=False` prevents pandas from adding an unnecessary index column.\n",
        "        try:\n",
        "            df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "            print(f\"   (3/3) Loaded data into the '{table_name}' table in the database.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading data for '{file_name}' into table '{table_name}': {e}\")\n",
        "            # If an error occurs during to_sql, attempt to rollback the transaction\n",
        "            # This is important to reset the connection state for the next iteration\n",
        "            try:\n",
        "                with engine.connect() as connection:\n",
        "                    connection.rollback()\n",
        "                print(f\"   Rolled back transaction for '{table_name}'.\")\n",
        "            except Exception as rollback_e:\n",
        "                 print(f\"   Error during rollback for '{table_name}': {rollback_e}\")\n",
        "\n",
        "\n",
        "# --- 4. VERIFICATION ---\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n--- ETL Process Finished. Total time: {end_time - start_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRlud__8FYH6",
        "outputId": "319d0bf3-f471-49a4-8f0e-1b1487811a9e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing file: 'olist_order_payments_dataset.csv'...\n",
            "   (1/3) Extracted 103886 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'order_payments' table in the database.\n",
            "\n",
            "Processing file: 'olist_sellers_dataset.csv'...\n",
            "   (1/3) Extracted 3095 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'sellers' table in the database.\n",
            "\n",
            "Processing file: 'product_category_name_translation.csv'...\n",
            "   (1/3) Extracted 71 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'product_category_translation' table in the database.\n",
            "\n",
            "Processing file: 'olist_order_reviews_dataset.csv'...\n",
            "   (1/3) Extracted 99224 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'order_reviews' table in the database.\n",
            "\n",
            "Processing file: 'olist_customers_dataset.csv'...\n",
            "   (1/3) Extracted 99441 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'customers' table in the database.\n",
            "\n",
            "Processing file: 'olist_products_dataset.csv'...\n",
            "   (1/3) Extracted 32951 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'products' table in the database.\n",
            "\n",
            "Processing file: 'olist_geolocation_dataset.csv'...\n",
            "   (1/3) Extracted 1000163 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'geolocation' table in the database.\n",
            "\n",
            "Processing file: 'olist_orders_dataset.csv'...\n",
            "   (1/3) Extracted 99441 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'orders' table in the database.\n",
            "\n",
            "Processing file: 'olist_order_items_dataset.csv'...\n",
            "   (1/3) Extracted 112650 rows.\n",
            "   (2/3) Transformed data types for date columns.\n",
            "   (3/3) Loaded data into the 'order_items' table in the database.\n",
            "\n",
            "--- ETL Process Finished. Total time: 121.77 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 4: Verifying that all tables were created in the database...\")\n",
        "try:\n",
        "    inspector = inspect(engine)\n",
        "    tables_in_db = inspector.get_table_names()\n",
        "    print(\"Verification successful. The following tables now exist:\")\n",
        "    for table_name in tables_in_db:\n",
        "        print(f\"  - {table_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not verify tables. An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofQYyFFZGduy",
        "outputId": "c17d345e-0d13-461a-b858-665d7bec4776"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Verifying that all tables were created in the database...\n",
            "Verification successful. The following tables now exist:\n",
            "  - products\n",
            "  - customers\n",
            "  - geolocation\n",
            "  - orders\n",
            "  - order_items\n",
            "  - product_recommendations\n",
            "  - order_payments\n",
            "  - sellers\n",
            "  - product_category_translation\n",
            "  - order_reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sql(query, engine):\n",
        "    \"\"\"\n",
        "    Executes a SQL query and returns the result as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Executing Query:\\n---\\n{query.strip()}\\n---\")\n",
        "    return pd.read_sql_query(query, engine)"
      ],
      "metadata": {
        "id": "2bQKWdKBJpJn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Question 1: What are the top 10 best-selling products this month?**"
      ],
      "metadata": {
        "id": "118XmOwsJ1hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import text\n",
        "\n",
        "print(\"--- Running Diagnostic Checks on the 'orders' table ---\")\n",
        "\n",
        "# Diagnostic Query 1: Check the data type of the timestamp column in the DB\n",
        "dtype_query = \"\"\"\n",
        "SELECT data_type\n",
        "FROM information_schema.columns\n",
        "WHERE table_name = 'orders' AND column_name = 'order_purchase_timestamp';\n",
        "\"\"\"\n",
        "\n",
        "# Diagnostic Query 2: Find the MIN and MAX dates to see the range\n",
        "date_range_query = \"\"\"\n",
        "SELECT\n",
        "    MIN(order_purchase_timestamp) as min_date,\n",
        "    MAX(order_purchase_timestamp) as max_date,\n",
        "    COUNT(*) as total_rows,\n",
        "    COUNT(order_purchase_timestamp) as non_null_dates\n",
        "FROM orders;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Execute and print results\n",
        "    df_dtype = pd.read_sql(text(dtype_query), engine)\n",
        "    print(\"\\n1. Data Type of 'order_purchase_timestamp':\")\n",
        "    print(df_dtype)\n",
        "\n",
        "    df_range = pd.read_sql(text(date_range_query), engine)\n",
        "    print(\"\\n2. Date Range and Null Count in 'order_purchase_timestamp':\")\n",
        "    print(df_range)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during diagnostics: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvS0yUyoeETB",
        "outputId": "c02a3fd7-fa0d-4c7d-a9e4-f6d0e83c5dc4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Diagnostic Checks on the 'orders' table ---\n",
            "\n",
            "1. Data Type of 'order_purchase_timestamp':\n",
            "                     data_type\n",
            "0  timestamp without time zone\n",
            "\n",
            "2. Date Range and Null Count in 'order_purchase_timestamp':\n",
            "             min_date            max_date  total_rows  non_null_dates\n",
            "0 2016-09-04 21:15:19 2018-10-17 17:30:18       99441           99441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary SQLAlchemy\n",
        "# --- Business Question 1 (Revised): Top 10 Best-Selling Products in the last 3 full months---\n",
        "print(\"--- Query 1 (Revised): Top 10 Products (Last 3 Full Months) ---\")\n",
        "\n",
        "top_products_revised_query = \"\"\"\n",
        "SELECT\n",
        "    COALESCE(pc.product_category_name_english, p.product_category_name, 'Unknown') AS product_category,\n",
        "    COUNT(oi.order_id) AS units_sold\n",
        "FROM\n",
        "    order_items AS oi\n",
        "JOIN\n",
        "    orders AS o ON oi.order_id = o.order_id\n",
        "JOIN\n",
        "    products AS p ON oi.product_id = p.product_id\n",
        "LEFT JOIN\n",
        "    product_category_translation AS pc ON p.product_category_name = pc.product_category_name\n",
        "WHERE\n",
        "    o.order_purchase_timestamp >= (SELECT MAX(order_purchase_timestamp) FROM orders) - INTERVAL '3 months'\n",
        "    AND o.order_status = 'delivered'\n",
        "GROUP BY\n",
        "    product_category\n",
        "ORDER BY\n",
        "    units_sold DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_top_products = pd.read_sql(text(top_products_revised_query), engine)\n",
        "    print(df_top_products)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFl1Ykg2J3oW",
        "outputId": "bc6c476f-af38-4235-bb34-917d4cbe846c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.10)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (4.15.0)\n",
            "--- Query 1 (Revised): Top 10 Products (Last 3 Full Months) ---\n",
            "        product_category  units_sold\n",
            "0          health_beauty        1203\n",
            "1         bed_bath_table        1018\n",
            "2             housewares         948\n",
            "3         sports_leisure         734\n",
            "4        furniture_decor         704\n",
            "5          watches_gifts         681\n",
            "6  computers_accessories         655\n",
            "7                   auto         568\n",
            "8              telephony         413\n",
            "9              perfumery         341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Business Question 2: Who are the top 5 highest-value customers? ---\n",
        "print(\"\\n--- Query 2: Top 5 Highest-Value Customers (Lifetime Spend) ---\")\n",
        "\n",
        "top_customers_query = \"\"\"\n",
        "SELECT\n",
        "    c.customer_unique_id,\n",
        "    SUM(op.payment_value) AS lifetime_value,\n",
        "    COUNT(DISTINCT o.order_id) as total_orders\n",
        "FROM\n",
        "    customers c\n",
        "JOIN\n",
        "    orders o ON c.customer_id = o.customer_id\n",
        "JOIN\n",
        "    order_payments op ON o.order_id = op.order_id\n",
        "WHERE\n",
        "    o.order_status = 'delivered' -- Only count completed orders\n",
        "GROUP BY\n",
        "    c.customer_unique_id\n",
        "ORDER BY\n",
        "    lifetime_value DESC\n",
        "LIMIT 5;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_top_customers = pd.read_sql(text(top_customers_query), engine)\n",
        "    print(df_top_customers)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJLzW8TjBGV",
        "outputId": "750f4f5e-dac3-4188-928d-9a08b63822cd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Query 2: Top 5 Highest-Value Customers (Lifetime Spend) ---\n",
            "                 customer_unique_id  lifetime_value  total_orders\n",
            "0  0a0a92112bd4c708ca5fde585afaa872        13664.08             1\n",
            "1  da122df9eeddfedc1dc1f5349a1a690c         7571.63             2\n",
            "2  763c8b1c9c68a0229c42c9fc6f662b93         7274.88             1\n",
            "3  dc4802a71eae9be1dd28f5d788ceb526         6929.31             1\n",
            "4  459bef486812aa25204be022145caa62         6922.21             1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary SQLAlchemy\n",
        "\n",
        "print(\"--- Query 3: Market Basket Analysis (Top 10 Product Pairs) ---\")\n",
        "\n",
        "market_basket_query = \"\"\"\n",
        "/*\n",
        " This query finds pairs of products that are frequently bought in the same order.\n",
        " STEP 1: (product_pairs) Self-join order_items to find all pairs of products\n",
        "         within the same order. The condition 'a.product_id < b.product_id'\n",
        "         is a trick to prevent duplicate pairs (e.g., [A,B] and [B,A]).\n",
        " STEP 2: Join with product and category tables to get human-readable names.\n",
        " STEP 3: Group by the category pair and count the frequency.\n",
        "*/\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        a.order_id,\n",
        "        a.product_id AS product_a,\n",
        "        b.product_id AS product_b\n",
        "    FROM\n",
        "        order_items a\n",
        "    JOIN\n",
        "        order_items b ON a.order_id = b.order_id AND a.product_id < b.product_id\n",
        ")\n",
        "SELECT\n",
        "    COALESCE(pc1.product_category_name_english, 'Unknown') AS product_a_category,\n",
        "    COALESCE(pc2.product_category_name_english, 'Unknown') AS product_b_category,\n",
        "    COUNT(*) AS pair_frequency\n",
        "FROM\n",
        "    product_pairs pp\n",
        "JOIN\n",
        "    products p1 ON pp.product_a = p1.product_id\n",
        "JOIN\n",
        "    products p2 ON pp.product_b = p2.product_id\n",
        "LEFT JOIN\n",
        "    product_category_translation pc1 ON p1.product_category_name = pc1.product_category_name\n",
        "LEFT JOIN\n",
        "    product_category_translation pc2 ON p2.product_category_name = pc2.product_category_name\n",
        "GROUP BY\n",
        "    1, 2\n",
        "ORDER BY\n",
        "    pair_frequency DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_market_basket = pd.read_sql(text(market_basket_query), engine)\n",
        "    print(df_market_basket)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzPOyWYh6T_H",
        "outputId": "c17c7ec6-275a-4870-fcf5-bc2a30430eda"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.10)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (4.15.0)\n",
            "--- Query 3: Market Basket Analysis (Top 10 Product Pairs) ---\n",
            "         product_a_category        product_b_category  pair_frequency\n",
            "0            bed_bath_table            bed_bath_table            1147\n",
            "1           furniture_decor           furniture_decor             555\n",
            "2             health_beauty             health_beauty             347\n",
            "3     computers_accessories     computers_accessories             325\n",
            "4                housewares                housewares             306\n",
            "5             watches_gifts             watches_gifts             215\n",
            "6            sports_leisure            sports_leisure             189\n",
            "7              garden_tools              garden_tools             179\n",
            "8  fashion_bags_accessories  fashion_bags_accessories             154\n",
            "9                      toys                      toys             123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary SQLAlchemy\n",
        "\n",
        "print(\"\\n--- Query 4: Average Time Between 1st and 2nd Purchase ---\")\n",
        "\n",
        "time_between_purchases_query = \"\"\"\n",
        "/*\n",
        " This query calculates customer repurchase rate.\n",
        " STEP 1: (customer_order_rank) For each unique customer, rank their orders by date.\n",
        " STEP 2: (customer_pairs) Create pairs of consecutive orders (e.g., order 1 and 2,\n",
        "         order 2 and 3, etc.) for each customer.\n",
        " STEP 3: (purchase_gaps) Calculate the time difference between these consecutive orders.\n",
        " STEP 4: Calculate the average time gap specifically for the first-to-second purchase.\n",
        "*/\n",
        "WITH customer_order_rank AS (\n",
        "    SELECT\n",
        "        c.customer_unique_id,\n",
        "        o.order_id,\n",
        "        o.order_purchase_timestamp,\n",
        "        -- The ROW_NUMBER() window function assigns a rank to each order for a customer\n",
        "        ROW_NUMBER() OVER(PARTITION BY c.customer_unique_id ORDER BY o.order_purchase_timestamp) as order_rank\n",
        "    FROM\n",
        "        orders o\n",
        "    JOIN\n",
        "        customers c ON o.customer_id = c.customer_id\n",
        "    WHERE\n",
        "        o.order_status = 'delivered'\n",
        "),\n",
        "first_second_purchase AS (\n",
        "    SELECT\n",
        "        t1.customer_unique_id,\n",
        "        t1.order_purchase_timestamp AS first_purchase_date,\n",
        "        t2.order_purchase_timestamp AS second_purchase_date,\n",
        "        -- Calculate the difference between the two dates\n",
        "        t2.order_purchase_timestamp - t1.order_purchase_timestamp AS time_to_repurchase\n",
        "    FROM\n",
        "        customer_order_rank t1\n",
        "    JOIN\n",
        "        customer_order_rank t2 ON t1.customer_unique_id = t2.customer_unique_id\n",
        "    WHERE\n",
        "        t1.order_rank = 1 AND t2.order_rank = 2\n",
        ")\n",
        "SELECT\n",
        "    -- Calculate the final average and extract the number of days from the interval\n",
        "    AVG(time_to_repurchase) AS average_repurchase_time\n",
        "FROM\n",
        "    first_second_purchase;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_time_between = pd.read_sql(text(time_between_purchases_query), engine)\n",
        "    # The result might be a timedelta object, so let's format it nicely\n",
        "    if not df_time_between.empty and 'average_repurchase_time' in df_time_between.columns:\n",
        "         avg_time = df_time_between['average_repurchase_time'][0]\n",
        "         print(f\"The average time for a customer to make their second purchase is approximately: {avg_time.days} days.\")\n",
        "    else:\n",
        "        print(df_time_between)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3O_-LuJG0aD",
        "outputId": "1aeaf3d1-3c2b-49d1-cb21-01ca21b04c35"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.10)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (4.15.0)\n",
            "\n",
            "--- Query 4: Average Time Between 1st and 2nd Purchase ---\n",
            "The average time for a customer to make their second purchase is approximately: 81 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary SQLAlchemy\n",
        "print(\"--- Query 3 (Improved): Cross-Category Market Basket Analysis ---\")\n",
        "\n",
        "market_basket_improved_query = \"\"\"\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        a.order_id,\n",
        "        a.product_id AS product_a,\n",
        "        b.product_id AS product_b\n",
        "    FROM\n",
        "        order_items a\n",
        "    JOIN\n",
        "        order_items b ON a.order_id = b.order_id AND a.product_id < b.product_id\n",
        "),\n",
        "category_pairs AS (\n",
        "    SELECT\n",
        "        pp.order_id,\n",
        "        COALESCE(pc1.product_category_name_english, 'Unknown') AS product_a_category,\n",
        "        COALESCE(pc2.product_category_name_english, 'Unknown') AS product_b_category\n",
        "    FROM\n",
        "        product_pairs pp\n",
        "    JOIN\n",
        "        products p1 ON pp.product_a = p1.product_id\n",
        "    JOIN\n",
        "        products p2 ON pp.product_b = p2.product_id\n",
        "    LEFT JOIN\n",
        "        product_category_translation pc1 ON p1.product_category_name = pc1.product_category_name\n",
        "    LEFT JOIN\n",
        "        product_category_translation pc2 ON p2.product_category_name = pc2.product_category_name\n",
        ")\n",
        "SELECT\n",
        "    product_a_category,\n",
        "    product_b_category,\n",
        "    COUNT(*) AS pair_frequency\n",
        "FROM\n",
        "    category_pairs\n",
        "WHERE\n",
        "    -- THIS IS THE NEW CONDITION TO FIND CROSS-CATEGORY PAIRS\n",
        "    product_a_category != product_b_category\n",
        "GROUP BY\n",
        "    1, 2\n",
        "ORDER BY\n",
        "    pair_frequency DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    df_market_basket_improved = pd.read_sql(text(market_basket_improved_query), engine)\n",
        "    print(df_market_basket_improved)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpmkyqm_Lsah",
        "outputId": "d65d17a2-87ef-4e16-a98a-1ff2a75e3dda"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.10)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy) (4.15.0)\n",
            "--- Query 3 (Improved): Cross-Category Market Basket Analysis ---\n",
            "      product_a_category     product_b_category  pair_frequency\n",
            "0         bed_bath_table        furniture_decor              71\n",
            "1        furniture_decor         bed_bath_table              57\n",
            "2           home_confort         bed_bath_table              52\n",
            "3        furniture_decor           garden_tools              39\n",
            "4             housewares         bed_bath_table              27\n",
            "5                   baby                   toys              25\n",
            "6             housewares        furniture_decor              25\n",
            "7  computers_accessories                Unknown              24\n",
            "8           garden_tools        furniture_decor              21\n",
            "9            electronics  computers_accessories              19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn -q\n",
        "print(\"Installing/updating scikit-learn...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra-3R8GPXClc",
        "outputId": "591d2a85-8d59-4457-8d1e-653021b25018"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling/updating scikit-learn...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "print(\"libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVvPzrDrXHtp",
        "outputId": "8c98901f-f2c3-4619-fdeb-6c598831ef1a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. DATA EXTRACTION: Get interaction data from the database ---\n",
        "# We need to know which customers bought which products.\n",
        "print(\"\\nFetching user-item interaction data from the database...\")\n",
        "\n",
        "interaction_query = \"\"\"\n",
        "SELECT\n",
        "    c.customer_unique_id, -- Use the unique ID to track the person\n",
        "    oi.product_id,\n",
        "    COUNT(o.order_id) as purchase_count -- Could be used for weighting later\n",
        "FROM\n",
        "    orders o\n",
        "JOIN\n",
        "    customers c ON o.customer_id = c.customer_id\n",
        "JOIN\n",
        "    order_items oi ON o.order_id = oi.order_id\n",
        "WHERE\n",
        "    o.order_status = 'delivered'\n",
        "GROUP BY\n",
        "    1, 2;\n",
        "\"\"\"\n",
        "try:\n",
        "    df_interactions = pd.read_sql(text(interaction_query), engine)\n",
        "    print(f\"Successfully fetched {len(df_interactions)} interaction records.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Also, get a product lookup table for readable recommendations\n",
        "products_query = \"\"\"\n",
        "SELECT\n",
        "    p.product_id,\n",
        "    COALESCE(pc.product_category_name_english, p.product_category_name, 'Unknown') AS category\n",
        "FROM products p\n",
        "LEFT JOIN product_category_translation pc\n",
        "    ON p.product_category_name = pc.product_category_name;\n",
        "\"\"\"\n",
        "df_products = pd.read_sql(text(products_query), engine)\n",
        "print(df_products)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrjDxBEyZB0h",
        "outputId": "96a49516-40a1-4741-eb15-2b444d5ce933"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching user-item interaction data from the database...\n",
            "Successfully fetched 99785 interaction records.\n",
            "                             product_id                   category\n",
            "0      1e9e8ef04dbcff4541ed26657ea517e5                  perfumery\n",
            "1      3aa071139cb16b67ca9e5dea641aaa2f                        art\n",
            "2      96bd76ec8810374ed1b65e291975717f             sports_leisure\n",
            "3      cef67bcfe19066a932b7673e239eb23d                       baby\n",
            "4      9dc1a7de274444849c219cff195d0b71                 housewares\n",
            "...                                 ...                        ...\n",
            "32946  a0b7d5a992ccda646f2d34e418fff5a0            furniture_decor\n",
            "32947  bf4538d88321d0fd4412a93c974510e6  construction_tools_lights\n",
            "32948  9a7c6041fa9592d9d9ef6cfe62a71f8c             bed_bath_table\n",
            "32949  83808703fc0706a22e264b9d75f04a2e      computers_accessories\n",
            "32950  106392145fca363410d287a815be6de4             bed_bath_table\n",
            "\n",
            "[32951 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. DATA EXTRACTION: Get interaction data from the database ---\n",
        "# This part remains the same.\n",
        "print(\"\\nFetching user-item interaction data from the database...\")\n",
        "interaction_query = \"\"\"\n",
        "SELECT c.customer_unique_id, oi.product_id, 1 as purchased\n",
        "FROM orders o\n",
        "JOIN customers c ON o.customer_id = c.customer_id\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE o.order_status = 'delivered'\n",
        "GROUP BY 1, 2;\n",
        "\"\"\"\n",
        "try:\n",
        "    df_interactions = pd.read_sql(text(interaction_query), engine)\n",
        "    print(f\"Successfully fetched {len(df_interactions)} interaction records.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    raise\n",
        "# Also, get a product lookup table\n",
        "products_query = \"SELECT product_id, COALESCE(pc.product_category_name_english, p.product_category_name, 'Unknown') AS category FROM products p LEFT JOIN product_category_translation pc ON p.product_category_name = pc.product_category_name;\"\n",
        "df_products = pd.read_sql(text(products_query), engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfdrgJTnc9zY",
        "outputId": "42010ad4-be21-4769-b186-0e58f0c7afdf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching user-item interaction data from the database...\n",
            "Successfully fetched 99785 interaction records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. EFFICIENT DATA PREPARATION FOR MODELING ---\n",
        "print(\"\\nPreparing data and building the sparse matrix directly (memory-efficient method)...\")\n",
        "\n",
        "# We need to convert our product and user IDs into unique integer codes.\n",
        "df_interactions['product_id'] = df_interactions['product_id'].astype('category')\n",
        "df_interactions['customer_unique_id'] = df_interactions['customer_unique_id'].astype('category')\n",
        "\n",
        "product_codes = df_interactions['product_id'].cat.codes\n",
        "customer_codes = df_interactions['customer_unique_id'].cat.codes\n",
        "purchased_data = df_interactions['purchased'].values\n",
        "\n",
        "# Create the sparse matrix directly from the codes\n",
        "# Rows = product codes, Cols = customer codes, Data = purchased value (1)\n",
        "product_user_matrix_sparse = csr_matrix((purchased_data, (product_codes, customer_codes)))\n",
        "\n",
        "# Create lookup dictionaries to map back from codes to original IDs\n",
        "# This is crucial for our recommendation function\n",
        "code_to_product_id = {i: product for i, product in enumerate(df_interactions['product_id'].cat.categories)}\n",
        "code_to_customer_id = {i: customer for i, customer in enumerate(df_interactions['customer_unique_id'].cat.categories)}\n",
        "\n",
        "print(\"Sparse matrix built successfully. Shape:\", product_user_matrix_sparse.shape)\n",
        "# I have tried but, the thig is we are going to get like billions of elements inside it which cause\n",
        "# memory issues. I am using google colab free version which only provides limited memory. So I took\n",
        "# only limited data to check.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buIXvqLYVfuv",
        "outputId": "4939aa19-1133-4792-b05b-98639b18ddb0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing data and building the sparse matrix directly (memory-efficient method)...\n",
            "Sparse matrix built successfully. Shape: (32216, 93358)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. MODEL TRAINING ---\n",
        "print(\"\\nTraining the NearestNeighbors model...\")\n",
        "model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
        "model_knn.fit(product_user_matrix_sparse)\n",
        "print(\"Model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdYoJDJ5WOGP",
        "outputId": "891ee652-c51b-4084-c0a3-46679583d45f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the NearestNeighbors model...\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. CREATE THE RECOMMENDATION FUNCTION (ADAPTED FOR CODES) ---\n",
        "\n",
        "# We also need a map from the original product ID string to its new integer code\n",
        "product_id_to_code = {product: i for i, product in enumerate(df_interactions['product_id'].cat.categories)}\n",
        "\n",
        "def get_recommendations(product_id: str, model, matrix, n_recommendations: int = 5):\n",
        "    \"\"\"\n",
        "    Takes a product_id string and returns n recommended products.\n",
        "    \"\"\"\n",
        "    if product_id not in product_id_to_code:\n",
        "        return f\"Product ID '{product_id}' not found.\"\n",
        "\n",
        "    # Convert our product_id string to its internal integer code\n",
        "    product_code = product_id_to_code[product_id]\n",
        "\n",
        "    distances, indices = model.kneighbors(matrix[product_code], n_neighbors=n_recommendations + 1)\n",
        "\n",
        "    recommendations = []\n",
        "    for i in range(1, len(distances.squeeze())):\n",
        "        recommended_code = indices.squeeze()[i]\n",
        "        recommended_product_id = code_to_product_id[recommended_code] # Map back to original ID\n",
        "        similarity_score = 1 - distances.squeeze()[i]\n",
        "\n",
        "        product_details = df_products[df_products['product_id'] == recommended_product_id]\n",
        "        category = product_details['category'].iloc[0] if not product_details.empty else 'N/A'\n",
        "\n",
        "        recommendations.append({\n",
        "            'product_id': recommended_product_id,\n",
        "            'category': category,\n",
        "            'similarity': f\"{similarity_score:.4f}\"\n",
        "        })\n",
        "    return pd.DataFrame(recommendations)\n"
      ],
      "metadata": {
        "id": "NBaVndxmWUDn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. EXAMPLE USAGE ---\n",
        "most_popular_product_id = df_interactions['product_id'].value_counts().index[0]\n",
        "\n",
        "print(f\"\\n--- Recommendations for a popular product (ID: {most_popular_product_id}) ---\")\n",
        "recommendations_df = get_recommendations(most_popular_product_id, model_knn, product_user_matrix_sparse)\n",
        "print(recommendations_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHpXbPCQccZE",
        "outputId": "3292e677-1f0b-4af5-b18a-009c744ffece"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Recommendations for a popular product (ID: 99a4788cb24856965c36a24e339b6058) ---\n",
            "                         product_id         category similarity\n",
            "0  35afc973633aaeb6b877ff57b2793310     home_confort     0.1098\n",
            "1  f2e53dd1670f3c376518263b3f71424d   bed_bath_table     0.0531\n",
            "2  b8be9a5f11908b7b70329f92fcb0eec9  furniture_decor     0.0468\n",
            "3  b05fae603a3a28a977633c139cece058   bed_bath_table     0.0468\n",
            "4  7aef20f12c90de905bf7782cb29dffb3  furniture_decor     0.0468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 5: EVALUATION - PRE-CALCULATING & STORING RECOMMENDATIONS (Corrected)\n",
        "# ==============================================================================\n",
        "# This version includes robust error handling for the database loading step\n",
        "# to correctly manage transaction rollbacks.\n",
        "\n",
        "import pandas as pd\n",
        "print(\"Starting recommendation pre-calculation...\")\n",
        "\n",
        "# --- 1. Identify Top N Products ---\n",
        "# (Assuming previous code block has run, otherwise re-run to define df_interactions etc.)\n",
        "N = 1000\n",
        "try:\n",
        "    top_products = df_interactions['product_id'].value_counts().head(N).index.tolist()\n",
        "    print(f\"Identified the top {len(top_products)} most popular products.\")\n",
        "except NameError:\n",
        "    print(\"Error: 'df_interactions' not found. Please re-run the previous model training cell.\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Calculate Recommendations for Each Top Product ---\n",
        "all_recommendations = []\n",
        "print(f\"Calculating recommendations for each of the top {N} products... (This may take a minute)\")\n",
        "\n",
        "for i, product_id in enumerate(top_products):\n",
        "    recs_df = get_recommendations(product_id, model_knn, product_user_matrix_sparse, n_recommendations=5)\n",
        "    if isinstance(recs_df, pd.DataFrame) and not recs_df.empty:\n",
        "        recs_df['source_product_id'] = product_id\n",
        "        all_recommendations.append(recs_df)\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"   ...processed {i + 1} / {N} products.\")\n",
        "\n",
        "# --- 3. Compile and Format the Final DataFrame ---\n",
        "if all_recommendations:\n",
        "    final_recommendations_df = pd.concat(all_recommendations, ignore_index=True)\n",
        "    final_recommendations_df = final_recommendations_df[['source_product_id', 'product_id', 'category', 'similarity']]\n",
        "    final_recommendations_df.rename(columns={'product_id': 'recommended_product_id'}, inplace=True)\n",
        "    print(f\"\\nSuccessfully generated a total of {len(final_recommendations_df)} recommendations.\")\n",
        "    print(\"Sample of the recommendations table to be loaded:\")\n",
        "    print(final_recommendations_df.head())\n",
        "\n",
        "    # --- 4. Load Recommendations into the Database (Robust Method) ---\n",
        "    print(\"\\nLoading pre-calculated recommendations into the database...\")\n",
        "\n",
        "    # Establish a new connection to ensure a clean transaction state\n",
        "    with engine.connect() as connection:\n",
        "        try:\n",
        "            # Begin a transaction\n",
        "            with connection.begin():\n",
        "                final_recommendations_df.to_sql(\n",
        "                    'product_recommendations',\n",
        "                    connection,  # Load using the connection, not the engine\n",
        "                    if_exists='replace',\n",
        "                    index=False\n",
        "                )\n",
        "            print(\"‚úÖ Successfully created and populated the 'product_recommendations' table.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during database load: {e}\")\n",
        "            # The 'with connection.begin()' context manager automatically handles rollback on error.\n",
        "            print(\"   Transaction has been rolled back.\")\n",
        "\n",
        "else:\n",
        "    print(\"No recommendations were generated.\")\n",
        "\n",
        "# Let's verify the table exists after the operation\n",
        "from sqlalchemy import inspect\n",
        "inspector = inspect(engine)\n",
        "if 'product_recommendations' in inspector.get_table_names():\n",
        "    print(\"\\nVerification successful: 'product_recommendations' table found in the database.\")\n",
        "else:\n",
        "    print(\"\\nVerification failed: 'product_recommendations' table was not created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzVMPY8_eQlO",
        "outputId": "faea8846-c70e-4ffb-f07d-1a335ec98d7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting recommendation pre-calculation...\n",
            "Identified the top 1000 most popular products.\n",
            "Calculating recommendations for each of the top 1000 products... (This may take a minute)\n",
            "   ...processed 100 / 1000 products.\n",
            "   ...processed 200 / 1000 products.\n",
            "   ...processed 300 / 1000 products.\n",
            "   ...processed 400 / 1000 products.\n",
            "   ...processed 500 / 1000 products.\n",
            "   ...processed 600 / 1000 products.\n",
            "   ...processed 700 / 1000 products.\n",
            "   ...processed 800 / 1000 products.\n",
            "   ...processed 900 / 1000 products.\n",
            "   ...processed 1000 / 1000 products.\n",
            "\n",
            "Successfully generated a total of 5000 recommendations.\n",
            "Sample of the recommendations table to be loaded:\n",
            "                  source_product_id            recommended_product_id  \\\n",
            "0  99a4788cb24856965c36a24e339b6058  35afc973633aaeb6b877ff57b2793310   \n",
            "1  99a4788cb24856965c36a24e339b6058  f2e53dd1670f3c376518263b3f71424d   \n",
            "2  99a4788cb24856965c36a24e339b6058  b8be9a5f11908b7b70329f92fcb0eec9   \n",
            "3  99a4788cb24856965c36a24e339b6058  b05fae603a3a28a977633c139cece058   \n",
            "4  99a4788cb24856965c36a24e339b6058  7aef20f12c90de905bf7782cb29dffb3   \n",
            "\n",
            "          category similarity  \n",
            "0     home_confort     0.1098  \n",
            "1   bed_bath_table     0.0531  \n",
            "2  furniture_decor     0.0468  \n",
            "3   bed_bath_table     0.0468  \n",
            "4  furniture_decor     0.0468  \n",
            "\n",
            "Loading pre-calculated recommendations into the database...\n",
            "‚úÖ Successfully created and populated the 'product_recommendations' table.\n",
            "\n",
            "Verification successful: 'product_recommendations' table found in the database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing necessary libraries...\")\n",
        "!pip install gspread gspread-dataframe google-auth-oauthlib -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kYPIx7HrMtK",
        "outputId": "3b319c73-48cb-4092-d444-f2214c42631b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# This is the key change: We use the 'drive' library which forces a robust auth flow.\n",
        "# It also has the side benefit of mounting your Google Drive to the Colab environment.\n",
        "print(\"\\nAuthorizing this notebook to access your Google account...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After mounting the drive, the credentials are in a state that gspread can use.\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "print(\"\\n‚úÖ Authentication and Drive mount successful.\")\n",
        "print(\"   You can now access your Google Drive files in the '/content/drive/My Drive/' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goY4RYjhqxts",
        "outputId": "3d1de00c-5ff1-46fb-cb20-9ede5633456f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Authorizing this notebook to access your Google account...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "‚úÖ Authentication and Drive mount successful.\n",
            "   You can now access your Google Drive files in the '/content/drive/My Drive/' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import text\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "print(\"--- Starting Corrected Data Export ---\")\n",
        "\n",
        "# Connect to Google Sheets (assuming 'gc' is from the auth cell)\n",
        "try:\n",
        "    sh = gc.open('E-commerce Dashboard Data')\n",
        "    print(\"Successfully opened the Google Sheet.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not open the Google Sheet. Please re-run the authentication cell. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define the CORRECTED query with order_id\n",
        "corrected_sales_query = \"\"\"\n",
        "SELECT\n",
        "    o.order_id, -- <<< THE MISSING FIELD\n",
        "    o.order_purchase_timestamp,\n",
        "    op.payment_value,\n",
        "    COALESCE(pc.product_category_name_english, p.product_category_name, 'Unknown') as product_category\n",
        "FROM\n",
        "    orders o\n",
        "JOIN\n",
        "    order_payments op ON o.order_id = op.order_id\n",
        "JOIN\n",
        "    order_items oi ON o.order_id = oi.order_id\n",
        "JOIN\n",
        "    products p ON oi.product_id = p.product_id\n",
        "LEFT JOIN\n",
        "    product_category_translation pc ON p.product_category_name = pc.product_category_name\n",
        "WHERE\n",
        "    o.order_status = 'delivered';\n",
        "\"\"\"\n",
        "print(\"Running corrected SQL query...\")\n",
        "df_sales_corrected = pd.read_sql(text(corrected_sales_query), engine)\n",
        "print(f\"Fetched {len(df_sales_corrected)} rows with the 'order_id' column.\")\n",
        "\n",
        "try:\n",
        "    # Delete the old worksheet to replace it\n",
        "    worksheet_to_delete = sh.worksheet('Sales_Overview')\n",
        "    sh.del_worksheet(worksheet_to_delete)\n",
        "    print(\"Deleted old 'Sales_Overview' tab.\")\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    print(\"Old 'Sales_Overview' tab not found, will create a new one.\")\n",
        "\n",
        "\n",
        "# Create a new, clean worksheet and load the corrected data\n",
        "ws_sales_new = sh.add_worksheet(rows=df_sales_corrected.shape[0]+1, cols=df_sales_corrected.shape[1], title=\"Sales_Overview\")\n",
        "set_with_dataframe(ws_sales_new, df_sales_corrected)\n",
        "\n",
        "print(\"\\n‚úÖ‚úÖ‚úÖ SUCCESSFULLY FIXED: The 'Sales_Overview' tab in your Google Sheet now contains the correct data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "qfBj3ubmsUyZ",
        "outputId": "8b2ce412-0d78-4918-9de3-95a547b65743"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Corrected Data Export ---\n",
            "Could not open the Google Sheet. Please re-run the authentication cell. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x793d3ac8dd60>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RefreshError",
          "evalue": "(\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x793d3ac8dd60>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTransportError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             self.token, self.expiry = _metadata.get_service_account_token(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36m_retrieve_info\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         info = _metadata.get_service_account_info(\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_account_email\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\u001b[0m in \u001b[0;36mget_service_account_info\u001b[0;34m(request, service_account)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# for more on the use of 'recursive'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"recursive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(request, path, root, params, recursive, retry_count, headers, return_none_for_not_found_error)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     raise exceptions.TransportError(\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;34m\"Failed to retrieve {} from the Google Compute Engine \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTransportError\u001b[0m: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x793d3ac8dd60>)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1012230368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Connect to Google Sheets (assuming 'gc' is from the auth cell)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E-commerce Dashboard Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Successfully opened the Google Sheet.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gspread/client.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, title, folder_id)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My fancy spreadsheet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mspreadsheet_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_spreadsheet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             properties = finditem(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gspread/client.py\u001b[0m in \u001b[0;36m_list_spreadsheet_files\u001b[0;34m(self, title, folder_id)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pageToken\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"files\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gspread/http_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     ) -> Response:\n\u001b[0;32m--> 114\u001b[0;31m         response = self.session.request(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mremaining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36mbefore_request\u001b[0;34m(self, request, method, url, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_metric_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_header_for_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\u001b[0m in \u001b[0;36m_blocking_refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_non_blocking_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mnew_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRefreshError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaught_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRefreshError\u001b[0m: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x793d3ac8dd60>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I am so confused about this issuee. And I did everything manually like shifting the data to csv files\n",
        "# and I built the dashboard using lookerstudio by google which free and beginner friendly also.\n",
        "#I will share the link in github. You can check there and suggest me if there is any way to improve it."
      ],
      "metadata": {
        "id": "uYV7O6Gtin0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}